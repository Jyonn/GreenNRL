name: LLAMA-MINER.D${model.config.hidden_size}.L${model.config.item_config.layer_split}.Lora${model.config.item_config.lora}
meta:
  item: Llama
  user: PolyAttention
  predictor: Dot
config:
  use_item_content: true
  max_item_content_batch_size: ${max_item_batch_size}$
  same_dim_transform: false
  embed_hidden_size: ${embed_hidden_size}$
  hidden_size: ${hidden_size}$
  neg_count: 4
  item_config:
    llm_dir: /home/data1/qijiong/llama-${llm_ver}
    layer_split: ${layer}$
    lora: ${lora}$
    weights_dir: data/${data.name}/llama-${llm_ver}-split
  user_config:
    num_context_codes: 32
    context_code_dim: 256
    inputer_config:
      use_cls_token: false
      use_sep_token: false
