name: PLMNR-DIN.D${hidden_size}
meta:
  item: Transformer
  user: NullConcat
  predictor: DIN
config:
  use_item_content: true
  use_neg_sampling: false
  hidden_size: ${hidden_size}$
  dnn_hidden_units: [ 1000, 1000, 1000 ]
  dnn_activations: ReLU
  attention_hidden_units: [ 64 ]
  attention_dropout: 0.1
  net_dropout: 0.1
  batch_norm: false
  din_use_softmax: false
  item_config:
    num_attention_heads: 8
    num_hidden_layers: 3
    inputer_config:
      use_cls_token: false
      use_sep_token: true
  user_config:
    inputer_config:
      use_cls_token: false
      use_sep_token: false
    flatten: false
    max_pooling: false
  predictor_config:
    dnn_hidden_units: [ 1000, 1000, 1000 ]
    dnn_activations: ReLU
    attention_hidden_units: [ 64 ]
    attention_dropout: 0.1
    net_dropout: 0.1
    batch_norm: false
    din_use_softmax: false
